<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 19.7 双向循环神经网络

### 19.7.1 深度循环神经网络的结构图

前面学习的内容，都是因为“过去”的时间步的状态对“未来”的时间步的状态有影响，才产生了不同种类的循环神经网络结构，在本节中，我们将学习一种双向影响的结构，即双向循环神经网络。

比如在一个语音识别的模型中，可能前面的一个词听上去比较模糊，会产生多个猜测，但是后面的词都很清晰，于是可以用后面的词来为前面的词提供一个最有把握（概率最大）的猜测。

<img src="../Images/19/bi_rnn_net.png"/>

图19-25 双向循环神经网络结构图

在图中，用h1、s1表示正向计算的状态，h2、s2表示反向计算的状态。而U、V、W三个参数矩阵也有就对应的变化，从1个变成了1对，并不共享参数。

另外请注意，图19-25仍然是一个有向无环图，仔细沿着每一个箭头搜索，会发现并不会产生首尾相连的封闭状态，这里的“封闭”与循环神经网络中的“循环”的含义不同。

所以在本节中会出现两组相似的词：前向、反向、正向、逆向。区别如下：

- 前向：是指神经网络中通常所说的前向计算。
- 反向：是指神经网络中通常所说的反向传播。
- 正向：是指双向循环神经网络中的从左到右时间步。在正向过程中，会存在前向计算和反向传播。
- 逆向：是指双向循环神经网络中的从右到左时间步。在逆向过程中，也会存在前向计算和反向传播。

### 19.7.2 前向计算

我们先假设应用场景只需要在最后一个时间步有输出（比如19.4节和19.5节中的应用就是如此），所以t2所代表的所有中间步都没有a、loss、y三个节点（用空心的圆表示），只有最后一个时间步有输出。

与前面的单向循环网络不同的是，由于有逆向网络的存在，在逆向过程中，t3是第一个时间步，t1是最后一个时间步，所以t1也应该有输出。

#### 公式推导

$$
h1 = x \cdot U1 + s1_{t-1} \cdot W1 \tag{1}
$$

注意公式1在t1时，$s1_{t-1}$是空，所以加法的第二项不存在。

$$
s1 = Tanh(h1) \tag{2}
$$

$$
h2 = x \cdot U2 + s2_{t+1} \cdot W2 \tag{3}
$$

注意公式3在t3时，$s2_{t+1}$是空，所以加法的第二项不存在。

$$
s2 = Tanh(h2) \tag{4}
$$

$$
z = s1 \cdot V1 + s2 \cdot V2 \tag{5}
$$

$$
a = Softmax(z) \tag{6}
$$

公式6只对t1、t3有效，对于所有其他中间的时间步来说，不需要计算。

#### 代码实现

由于是双向的，所以在主过程中，存在一正一反两个计算链。

```Python
class timestep(object):
    def forward_f2e(self, x, U1, U2, V1, V2, W1, W2, prev_s1, isFirst):
        ......
        # 公式1
        if (isFirst):
            self.h1 = np.dot(x, U1)
        else:
            self.h1 = np.dot(x, U1) + np.dot(prev_s1, W1) 
        # 公式2
        self.s1 = Tanh().forward(self.h1)

    def forward_e2f(self, next_s2, isFirst, isLast):
        # 公式3
        if (isLast):
            self.h2 = np.dot(self.x, self.U2)
        else:
            self.h2 = np.dot(self.x, self.U2) + np.dot(next_s2, self.W2)
        # 公式4
        self.s2 = Tanh().forward(self.h2)

        if (isFirst or isLast):
            # 公式5
            self.z = np.dot(self.s1, self.V1) + np.dot(self.s2, self.V2)
            # 公式6
            self.a = Softmax().forward(self.z)
```
正向的前向计算函数叫做forward_f2e，意为front to end，即从前向后的方向；逆向的前向计算函数叫做forward_e2f，意为end to front，即从后向前的方向。

注意对输出部分的计算，即节点z和a，只有在第一个和最后一个节点才发生（本应用场景的需要），而且只能在forward_e2f函数里才能出现，因为此时s1和s2的值都计算好了，而在forward_f2e函数中，s2的值并没有计算好。

### 19.7.3 反向传播

#### 正向计算链的反向传播

先推导正向计算链的反向传播公式，即关于h1、s1节点的计算。

对于最后一个时间步（t3）：

$$
\frac{\partial loss_\tau}{\partial z_\tau}=a_\tau-y_\tau \rightarrow dz_\tau \tag{7}
$$

$$
\frac{\partial loss_\tau}{\partial h1_\tau}=\frac{\partial loss_\tau}{\partial z_\tau}\frac{\partial z_\tau}{\partial s1_\tau}\frac{\partial s1_\tau}{\partial h1_\tau}=dz_\tau \cdot V1^T \odot \sigma'(s1_\tau) \rightarrow dh1_\tau \tag{8}
$$

其中$\sigma'(s1)$表示激活函数的导数，$s1$是激活函数的数值。下同。

对于中间的所有时间步：

$$
\begin{aligned}
\frac{\partial J}{\partial h1_t} &= \frac{\partial loss_t}{\partial z_t}\frac{\partial z_t}{\partial s1_t}\frac{\partial s1_t}{\partial h1_t} + \frac{\partial loss_{t+1}}{\partial h1_{t+1}}\frac{\partial h1_{t+1}}{\partial s1_{t}}\frac{\partial s1_t}{\partial h1_t}
\\
&=dz_t \cdot V1^T \odot \sigma'(s1_t) + \frac{\partial loss_{t+1}}{\partial h1_{t+1}} \cdot W1^T \odot \sigma'(s1_t)
\\
&=(dz_t \cdot V1^T + dh1_{t+1} \cdot W1^T) \odot \sigma'(s1_t) \rightarrow dh1_t
\end{aligned} \tag{9}
$$

对于$V1$来说，只有当前时间步的损失函数会给它反向传播的误差，与别的时间步没有关系，所以有：

$$
\frac{\partial loss_t}{\partial V1_t} = \frac{\partial loss_t}{\partial z_t}\frac{\partial z_t}{\partial V1_t}= s1_t^T \cdot dz_t \rightarrow dV1 \tag{10}
$$

对于$U1$，后面的时间步都会给它反向传播误差，但是我们只从$h1$节点考虑：

$$
\frac{\partial J}{\partial U1_t} = \frac{\partial J}{\partial h1_t}\frac{\partial h1_t}{\partial U1_t}= x^T_t \cdot dh1_t \rightarrow dU1 \tag{11}
$$

对于$W1$，和$U1$的考虑是一样的：

$$
\frac{\partial J}{\partial W1_t} = \frac{\partial J}{\partial h1_t}\frac{\partial h1_t}{\partial W1_t}= s1_{t-1}^T \cdot dh1_t \rightarrow dW1 \tag{16}
$$

对于第一个时间步，$s1_{t-1}$不存在，所以没有$dW1$：

$$
dW1 = 0 \tag{17}
$$


公式9中的$dh1_{t+1}$，就是上一步中计算得到的$dh1_t$，如果追溯到最开始，即公式8中的$dh1$。因此，先有最后一个时间步的$dh1$，然后依次向前推，就可以得到所有时间步的$dh1_t$。

#### 逆向计算链的反向传播

下面推导反向计算链的反向传播公式，即关于h2、s2节点的计算。

对于第一个时间步（t1，实际上是逆向计算的最后一个时间步）：

$$
\frac{\partial loss_\tau}{\partial h2_\tau}=\frac{\partial loss_\tau}{\partial z_\tau}\frac{\partial z_\tau}{\partial s2_\tau}\frac{\partial s2_\tau}{\partial h2_\tau}=dz_\tau \cdot (V2)^T \odot \sigma'(s2_\tau) \rightarrow dh2_\tau \tag{10}
$$

对于中间的所有时间步：

$$
\begin{aligned}
\frac{\partial J}{\partial h2_t} &= \frac{\partial loss_t}{\partial z_t}\frac{\partial z_t}{\partial s2_t}\frac{\partial s2_t}{\partial h2_t} + \frac{\partial J}{\partial h2_{t-1}}\frac{\partial h2_{t-1}}{\partial s2_{t}}\frac{\partial s2_t}{\partial h2_t}
\\
&=dz_t \cdot V2^T \odot \sigma'(s2_t) + \frac{\partial J}{\partial h2_{t-1}} \cdot W2^T \odot \sigma'(s2_t)
\\
&=(dz_t \cdot V2^T + dh2_{t-1} \cdot W2^T) \odot \sigma'(s2_t) \rightarrow dh2_t
\end{aligned} \tag{11}
$$


$$
\frac{\partial loss_t}{\partial V2_t} = \frac{\partial loss_t}{\partial z_t}\frac{\partial z_t}{\partial V2_t}= s2^T_t \cdot dz_t \rightarrow dV2 \tag{13}
$$



$$
\frac{\partial J}{\partial U2_t} = \frac{\partial J}{\partial h2_t}\frac{\partial h2_t}{\partial U2_t}= x^T_t \cdot dh2_t \rightarrow dU2 \tag{15}
$$

$$
\frac{\partial J}{\partial W2_t} = \frac{\partial J}{\partial h2_t}\frac{\partial h2_t}{\partial W2_t}= s2_{t+1}^T \cdot dh2_t \rightarrow dW2 \tag{18}
$$

对于最后一个时间步：

$$
dW2 = 0 \tag{19}
$$

### 19.7.4 代码实现

由于没找到合适的例子，所以只实现了网络基础部分的代码，并没有带入应用实例。所以请读者自行到代码库中阅读学习。

### 代码位置

ch19, Level7
